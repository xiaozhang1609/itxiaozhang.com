<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head>
	<meta name="generator" content="Hugo 0.152.2"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>IT小章</title>

<meta name="description" content="">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fd5566c526ae48aadabd950798a2dc3568536401560eae5caac3765a42a9e7b5.css" integrity="sha256-/VVmxSauSKravZUHmKLcNWhTZAFWDq5cqsN2WkKp57U=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/images/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/index.xml" title="rss">
<link rel="alternate" type="application/json" href="http://localhost:1313/index.json" title="json">
<link rel="alternate" hreflang="en" href="http://localhost:1313/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
</head>
<body class="list" id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="IT小章 (Alt + H)">IT小章</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="首页">
                    <span class="active">首页</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="归档">
                    <span>归档</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="标签">
                    <span>标签</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="搜索 (Alt &#43; /)" accesskey=/>
                    <span>搜索</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="关于">
                    <span>关于</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Python爬虫实战: 58同城房产信息自动化采集
    </h2>
  </header>
  <div class="entry-content">
    <p> 原文地址：https://itxiaozhang.com/python-web-crawler-58city-real-estate-data-collection/
如果您需要远程电脑维修或者编程开发，请加我微信咨询。
1. 简介 这是一个基于Python开发的58同城房产信息采集系统,主要用于自动采集商铺、写字楼、厂房和生意转让等房产信息。系统提供图形界面操作,支持多城市数据采集。
2. 主要功能 多城市房产信息采集 多线程并发处理 自动代理IP切换 数据实时入库 自动去重过滤 定时采集更新 3. 技术特点 多线程采集 使用Python threading实现并发采集,提高效率。
代理IP池 自动维护代理IP池,避免被反爬:
1 2 3 4 5 6 def roxies_ip(url): ip = requests.get(url).text proxies_ip_lists = [] for i in ip: proxies_ip_lists.append({&#39;https&#39;: &#34;//&#34; &#43; i}) return proxies_ip_lists[0] 数据存储 采用MySQL存储数据,支持实时入库和查重。
4. 使用方法 配置数据库信息 设置代理IP接口 选择目标城市和类型 设置采集间隔 启动采集任务 相关源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 &#34;&#34;&#34; ================================ 作者：IT小章 网站：itxiaozhang.com 时间：2024年12月01日 Copyright © 2024 IT小章 ================================ &#34;&#34;&#34; import threading import tkinter as tk from tkinter import ttk, messagebox import requests import time import json from bs4 import BeautifulSoup import logging from datetime import datetime import random import re from queue import Queue # 配置日志 logging.basicConfig( level=logging.INFO, format=&#39;%(asctime)s - %(levelname)s - %(message)s&#39; ) class Config: &#34;&#34;&#34;配置管理类&#34;&#34;&#34; # 示例城市配置 CITIES = { &#34;北京&#34;: {&#34;北京&#34;: &#34;bj|1&#34;}, &#34;上海&#34;: {&#34;上海&#34;: &#34;sh|2&#34;}, &#34;广州&#34;: {&#34;广州&#34;: &#34;gz|3&#34;}, &#34;深圳&#34;: {&#34;深圳&#34;: &#34;sz|4&#34;} } # 房产类型配置 HOUSE_TYPES = { &#34;商铺&#34;: &#34;/shangpucz/0/&#34;, &#34;写字楼&#34;: &#34;/zhaozu/0/&#34;, &#34;厂房&#34;: &#34;/changfang/0/&#34;, &#34;生意转让&#34;: &#34;/shengyizr/0/&#34; } @staticmethod def load_config(file_path=&#34;config.json&#34;): &#34;&#34;&#34;加载配置文件&#34;&#34;&#34; try: with open(file_path, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f: return json.load(f) except FileNotFoundError: logging.warning(f&#34;配置文件 {file_path} 不存在，使用默认配置&#34;) return {} class ProxyPool: &#34;&#34;&#34;代理IP池管理类&#34;&#34;&#34; def __init__(self): self.proxies = Queue() self.lock = threading.Lock() def add_proxy(self, proxy): &#34;&#34;&#34;添加代理&#34;&#34;&#34; self.proxies.put(proxy) def get_proxy(self): &#34;&#34;&#34;获取代理&#34;&#34;&#34; try: return self.proxies.get() except: return None def remove_proxy(self, proxy): &#34;&#34;&#34;移除失效代理&#34;&#34;&#34; with self.lock: if proxy in self.proxies.queue: self.proxies.queue.remove(proxy) class DataStorage: &#34;&#34;&#34;数据存储类&#34;&#34;&#34; def __init__(self): self.data_queue = Queue() def save(self, data): &#34;&#34;&#34;保存数据 实际使用时请实现具体的存储逻辑 &#34;&#34;&#34; logging.info(f&#34;保存数据: {data}&#34;) self.data_queue.put(data) class HouseCrawler: &#34;&#34;&#34;房产信息爬虫类&#34;&#34;&#34; def __init__(self): self.proxy_pool = ProxyPool() self.storage = DataStorage() self.headers = { &#34;User-Agent&#34;: &#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36&#34; } def get_page(self, url, retry_times=3): &#34;&#34;&#34;获取页面内容&#34;&#34;&#34; for _ in range(retry_times): try: proxy = self.proxy_pool.get_proxy() response = requests.get(url, headers=self.headers, proxies=proxy, timeout=10) response.encoding = &#39;utf-8&#39; if response.status_code == 200: return response.text except Exception as e: logging.error(f&#34;获取页面失败: {e}&#34;) if proxy: self.proxy_pool.remove_proxy(proxy) return None def parse_list_page(self, html): &#34;&#34;&#34;解析列表页&#34;&#34;&#34; if not html: return [] try: soup = BeautifulSoup(html, &#39;lxml&#39;) items = soup.select(&#39;ul .item&#39;) results = [] for item in items: try: title = item.select_one(&#39;.title&#39;).text.strip() link = item.select_one(&#39;.link&#39;)[&#39;href&#39;] results.append({ &#39;title&#39;: title, &#39;link&#39;: link }) except Exception as e: logging.error(f&#34;解析列表项失败: {e}&#34;) return results except Exception as e: logging.error(f&#34;解析列表页失败: {e}&#34;) return [] def parse_detail_page(self, html): &#34;&#34;&#34;解析详情页&#34;&#34;&#34; if not html: return None try: # 示例解析逻辑 data = { &#39;title&#39;: &#39;&#39;, &#39;price&#39;: &#39;&#39;, &#39;area&#39;: &#39;&#39;, &#39;location&#39;: &#39;&#39;, &#39;contact&#39;: &#39;&#39;, &#39;description&#39;: &#39;&#39; } soup = BeautifulSoup(html, &#39;lxml&#39;) # 实际使用时需要根据具体网页结构实现解析逻辑 return data except Exception as e: logging.error(f&#34;解析详情页失败: {e}&#34;) return None class GUI: &#34;&#34;&#34;图形界面类&#34;&#34;&#34; def __init__(self): self.root = tk.Tk() self.root.title(&#34;58同城房产信息采集系统 (学习版)&#34;) self.root.geometry(&#39;500x400&#39;) self.crawler = HouseCrawler() self.setup_gui() def setup_gui(self): &#34;&#34;&#34;设置GUI界面&#34;&#34;&#34; # 城市选择 tk.Label(self.root, text=&#34;选择城市:&#34;).grid(row=0, column=0, padx=5, pady=5) self.city_var = tk.StringVar() self.city_combo = ttk.Combobox(self.root, textvariable=self.city_var, state=&#34;readonly&#34;) self.city_combo[&#39;values&#39;] = list(Config.CITIES.keys()) self.city_combo.current(0) self.city_combo.grid(row=0, column=1, padx=5, pady=5) # 房产类型选择 tk.Label(self.root, text=&#34;房产类型:&#34;).grid(row=1, column=0, padx=5, pady=5) self.type_var = tk.StringVar() self.type_combo = ttk.Combobox(self.root, textvariable=self.type_var, state=&#34;readonly&#34;) self.type_combo[&#39;values&#39;] = list(Config.HOUSE_TYPES.keys()) self.type_combo.current(0) self.type_combo.grid(row=1, column=1, padx=5, pady=5) # 页数设置 tk.Label(self.root, text=&#34;采集页数:&#34;).grid(row=2, column=0, padx=5, pady=5) self.pages_var = tk.StringVar(value=&#34;1&#34;) self.pages_entry = tk.Entry(self.root, textvariable=self.pages_var) self.pages_entry.grid(row=2, column=1, padx=5, pady=5) # 间隔时间设置 tk.Label(self.root, text=&#34;间隔时间(秒):&#34;).grid(row=3, column=0, padx=5, pady=5) self.interval_var = tk.StringVar(value=&#34;5&#34;) self.interval_entry = tk.Entry(self.root, textvariable=self.interval_var) self.interval_entry.grid(row=3, column=1, padx=5, pady=5) # 状态显示 self.status_text = tk.Text(self.root, height=10, width=50) self.status_text.grid(row=4, column=0, columnspan=2, padx=5, pady=5) # 控制按钮 self.start_button = tk.Button(self.root, text=&#34;开始采集&#34;, command=self.start_crawl) self.start_button.grid(row=5, column=0, padx=5, pady=5) self.stop_button = tk.Button(self.root, text=&#34;停止采集&#34;, command=self.stop_crawl) self.stop_button.grid(row=5, column=1, padx=5, pady=5) # 采集状态 self.is_running = False def log_message(self, message): &#34;&#34;&#34;显示日志信息&#34;&#34;&#34; self.status_text.insert(tk.END, f&#34;{datetime.now().strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)} - {message}\n&#34;) self.status_text.see(tk.END) def start_crawl(self): &#34;&#34;&#34;开始采集&#34;&#34;&#34; if self.is_running: messagebox.showwarning(&#34;警告&#34;, &#34;采集任务正在进行中&#34;) return try: pages = int(self.pages_var.get()) interval = int(self.interval_var.get()) if pages &lt; 1 or interval &lt; 1: raise ValueError except ValueError: messagebox.showerror(&#34;错误&#34;, &#34;请输入有效的页数和间隔时间&#34;) return self.is_running = True threading.Thread(target=self.crawl_task, args=(pages, interval)).start() self.log_message(&#34;开始采集任务...&#34;) def stop_crawl(self): &#34;&#34;&#34;停止采集&#34;&#34;&#34; self.is_running = False self.log_message(&#34;正在停止采集任务...&#34;) def crawl_task(self, pages, interval): &#34;&#34;&#34;采集任务&#34;&#34;&#34; city = self.city_var.get() house_type = self.type_var.get() try: for page in range(1, pages &#43; 1): if not self.is_running: break self.log_message(f&#34;正在采集第 {page} 页...&#34;) url = self.generate_url(city, house_type, page) # 获取列表页 html = self.crawler.get_page(url) if not html: self.log_message(f&#34;获取第 {page} 页失败，跳过...&#34;) continue # 解析列表页 items = self.crawler.parse_list_page(html) self.log_message(f&#34;第 {page} 页发现 {len(items)} 条房源信息&#34;) # 处理每个房源 for item in items: if not self.is_running: break # 获取详情页 detail_html = self.crawler.get_page(item[&#39;link&#39;]) if detail_html: detail_data = self.crawler.parse_detail_page(detail_html) if detail_data: self.crawler.storage.save(detail_data) self.log_message(f&#34;成功采集: {item[&#39;title&#39;]}&#34;) # 间隔等待 time.sleep(interval) except Exception as e: self.log_message(f&#34;采集过程出错: {str(e)}&#34;) finally: self.is_running = False self.log_message(&#34;采集任务已完成&#34;) def generate_url(self, city, house_type, page): &#34;&#34;&#34;生成目标URL&#34;&#34;&#34; city_code = Config.CITIES[city][city].split(&#39;|&#39;)[0] type_path = Config.HOUSE_TYPES[house_type] return f&#34;https://{city_code}.58.com{type_path}pn{page}/&#34; def run(self): &#34;&#34;&#34;运行GUI&#34;&#34;&#34; self.root.mainloop() def main(): &#34;&#34;&#34;主程序入口&#34;&#34;&#34; try: # 启动GUI界面 app = GUI() app.run() except Exception as e: logging.error(f&#34;程序运行出错: {str(e)}&#34;) messagebox.showerror(&#34;错误&#34;, f&#34;程序运行出错: {str(e)}&#34;) if __name__ == &#34;__main__&#34;: &#34;&#34;&#34; 使用说明： 1. 安装依赖包： pip install requests beautifulsoup4 lxml 2. 运行程序： python crawler.py 3. 使用步骤： - 选择目标城市 - 选择房产类型 - 设置采集页数 - 设置采集间隔时间 - 点击&#34;开始采集&#34; 4. 注意事项： - 采集间隔建议设置在5秒以上 - 采集页数建议从小到大测试 - 如遇到错误，请查看日志信息 - 使用代理IP可以提高采集成功率 5. 数据存储： - 默认将数据打印到日志 - 可以修改DataStorage类实现其他存储方式 6. 代理设置： - 默认使用直连方式 - 可以修改ProxyPool类实现代理池功能 &#34;&#34;&#34; main() </p>
  </div>
  <footer class="entry-footer"><span title='2024-12-01 19:28:00 +0800 CST'>December 1, 2024</span>&nbsp;·&nbsp;<span>6 min</span>&nbsp;·&nbsp;<span>IT小章</span></footer>
  <a class="entry-link" aria-label="post link to Python爬虫实战: 58同城房产信息自动化采集" href="http://localhost:1313/python-web-crawler-58city-real-estate-data-collection/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Ubuntu下快速部署多个静态网站（含HTTPS配置）
    </h2>
  </header>
  <div class="entry-content">
    <p> 原文地址：https://itxiaozhang.com/ubuntu-multiple-static-sites-deployment-guide/
如果您需要远程电脑维修或者编程开发，请加我微信咨询。
环境 一台运行 Ubuntu 的服务器（本教程使用 Ubuntu 20.04） 已经购买的域名（支持配置多个域名） 一句话 教程分为两个主要步骤：第一步执行setup_static_sites.sh脚本，实现nginx安装和多站点的基础HTTP配置；第二步执行certbot相关命令，完成SSL证书申请和HTTPS的配置，
1. 准备部署脚本 创建文件 setup_static_sites.sh
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 #!/bin/bash # 配置区域 ==================== DOMAINS=( &#34;zhang9.com&#34; &#34;itxiaozhang.com&#34; ) WEB_ROOT=&#34;/var/www/html&#34; LOG_FILE=&#34;deployment_$(date &#43;%Y%m%d_%H%M%S).log&#34; # 工具函数 ==================== log() { echo &#34;[$(date &#39;&#43;%Y-%m-%d %H:%M:%S&#39;)] $1&#34; | tee -a &#34;$LOG_FILE&#34; } error() { log &#34;错误: $1&#34; exit 1 } # 配置单个域名 configure_domain() { local DOMAIN=$1 log &#34;配置域名: $DOMAIN&#34; local SITE_ROOT=&#34;$WEB_ROOT/$DOMAIN&#34; # 检查并创建网站目录 [ ! -d &#34;$SITE_ROOT&#34; ] &amp;&amp; mkdir -p &#34;$SITE_ROOT&#34; chown -R www-data:www-data &#34;$SITE_ROOT&#34; chmod -R 755 &#34;$SITE_ROOT&#34; # 配置Nginx（直接覆盖已存在的配置） cat &gt; &#34;/etc/nginx/sites-available/$DOMAIN&#34; &lt;&lt; EOF server { listen 80; listen [::]:80; server_name $DOMAIN www.$DOMAIN; root $SITE_ROOT; index index.html; location / { try_files \$uri \$uri/ =404; } access_log /var/log/nginx/$DOMAIN.access.log; error_log /var/log/nginx/$DOMAIN.error.log; } EOF # 启用配置 ln -sf &#34;/etc/nginx/sites-available/$DOMAIN&#34; &#34;/etc/nginx/sites-enabled/&#34; } # 主程序 main() { [ &#34;$(id -u)&#34; != &#34;0&#34; ] &amp;&amp; error &#34;需要root权限&#34; # 安装必要软件 apt update apt install -y nginx python3-certbot-nginx # 删除默认配置 rm -f &#34;/etc/nginx/sites-enabled/default&#34; # 配置每个域名 for domain in &#34;${DOMAINS[@]}&#34;; do configure_domain &#34;$domain&#34; done # 检查并重启Nginx nginx -t &amp;&amp; systemctl restart nginx # 输出说明 log &#34;配置完成！后续步骤:&#34; for domain in &#34;${DOMAINS[@]}&#34;; do log &#34;1. 上传网站文件到: $WEB_ROOT/$domain/&#34; done } main 2. 配置脚本权限并执行 1 2 chmod &#43;x setup_static_sites.sh sudo ./setup_static_sites.sh 3. 配置域名解析 在域名管理面板中添加以下记录：
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-11-29 13:30:16 +0800 CST'>November 29, 2024</span>&nbsp;·&nbsp;<span>2 min</span>&nbsp;·&nbsp;<span>IT小章</span></footer>
  <a class="entry-link" aria-label="post link to Ubuntu下快速部署多个静态网站（含HTTPS配置）" href="http://localhost:1313/ubuntu-multiple-static-sites-deployment-guide/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Python自动化工具：HMDB代谢物信息一键批量获取
    </h2>
  </header>
  <div class="entry-content">
    <p> 原文地址：https://itxiaozhang.com/python-hmdb-metabolites-batch-extractor/
如果您需要远程电脑维修或者编程开发，请加我微信咨询。
简介 这是一个用于从HMDB（人类代谢组数据库）批量提取代谢物信息的小工具。它可以帮助研究人员快速获取多个代谢物的详细信息，节省手动查询的时间。
功能特点 支持正负离子模式（Positive/Negative） 批量处理多个质量数 自动提取代谢物的关键信息 结果保存为Excel可直接打开的CSV文件 支持断点续传，出错自动重试 使用方法 准备输入文件：
positive.txt：存放正离子模式的质量数 negative.txt：存放负离子模式的质量数 每行一个质量数 运行程序：
双击运行程序 等待程序自动处理 完成后按回车键退出 查看结果：
程序会生成代谢物数据.csv文件 使用Excel打开即可查看所有提取的信息 输出结果包含 HMDB ID 离子模式 描述信息 分类信息（超类/类/子类） 来源信息 组织位置 相关数据库ID（KEGG/ChEBI/METLIN） 结构图链接 可增加其他字段 注意事项 请确保输入的质量数格式正确 程序运行需要网络连接 处理时间取决于数据量大小 部分代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 &#34;&#34;&#34; ================================ 作者：IT小章 网站：itxiaozhang.com 时间：2024年11月27日 Copyright © 2024 IT小章 ================================ &#34;&#34;&#34; import requests import re import csv import time import logging import os from concurrent.futures import ThreadPoolExecutor, as_completed from tqdm import tqdm from lxml import html # 配置日志 logging.basicConfig( level=logging.INFO, format=&#39;%(asctime)s - %(levelname)s - %(message)s&#39;, handlers=[ logging.FileHandler(&#34;代谢物提取.log&#34;, encoding=&#39;utf-8&#39;), logging.StreamHandler() ] ) class MetaboliteExtractor: &#34;&#34;&#34;代谢物数据提取器&#34;&#34;&#34; def __init__(self): self.base_url = &#34;https://hmdb.ca&#34; self.headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36&#39; } def process_files(self): &#34;&#34;&#34;处理输入文件&#34;&#34;&#34; try: logging.info(&#34;程序开始运行...&#34;) results = [] # 处理positive模式 if os.path.exists(&#39;positive.txt&#39;): logging.info(&#34;处理positive.txt...&#34;) results.extend(self._process_file(&#39;positive.txt&#39;, &#39;positive&#39;)) # 处理negative模式 if os.path.exists(&#39;negative.txt&#39;): logging.info(&#34;处理negative.txt...&#34;) results.extend(self._process_file(&#39;negative.txt&#39;, &#39;negative&#39;)) if results: self._save_results(results) logging.info(f&#34;处理完成，共获取 {len(results)} 条结果&#34;) else: logging.error(&#34;未找到任何结果&#34;) except Exception as e: logging.error(f&#34;处理过程出错: {str(e)}&#34;) finally: logging.info(&#34;程序运行结束&#34;) print(&#34;\n&#34; &#43; &#34;=&#34;*50) input(&#34;按回车键退出程序...&#34;) def _process_file(self, filename, mode): &#34;&#34;&#34;处理单个文件（具体实现已隐藏）&#34;&#34;&#34; try: with open(filename, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f: data = f.read().strip() if not data: logging.warning(f&#34;{filename} 为空&#34;) return [] logging.info(f&#34;正在处理 {filename}&#34;) return self._extract_data(data, mode) except FileNotFoundError: logging.error(f&#34;未找到文件: {filename}&#34;) return [] def _extract_data(self, data, mode): &#34;&#34;&#34;提取数据（具体实现已隐藏）&#34;&#34;&#34; # 核心实现已隐藏 pass def _save_results(self, results): &#34;&#34;&#34;保存结果到CSV&#34;&#34;&#34; try: filename = &#39;代谢物数据.csv&#39; with open(filename, &#39;w&#39;, newline=&#39;&#39;, encoding=&#39;utf-8-sig&#39;) as f: if results: writer = csv.DictWriter(f, fieldnames=results[0].keys()) writer.writeheader() writer.writerows(results) logging.info(f&#34;数据已保存到 {filename}&#34;) except Exception as e: logging.error(f&#34;保存数据失败: {str(e)}&#34;) def main(): extractor = MetaboliteExtractor() extractor.process_files() if __name__ == &#34;__main__&#34;: main() 视频版本 哔哩哔哩 YouTube </p>
  </div>
  <footer class="entry-footer"><span title='2024-11-27 12:08:15 +0800 CST'>November 27, 2024</span>&nbsp;·&nbsp;<span>2 min</span>&nbsp;·&nbsp;<span>IT小章</span></footer>
  <a class="entry-link" aria-label="post link to Python自动化工具：HMDB代谢物信息一键批量获取" href="http://localhost:1313/python-hmdb-metabolites-batch-extractor/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">阅读与感悟周记37
    </h2>
  </header>
  <div class="entry-content">
    <p> 原文地址：https://itxiaozhang.com/weekly-reflections-reading-insights-37/
如果您需要远程电脑维修或者编程开发，请加我微信咨询。
言论 我们每一个人都要感谢那几位勇敢无畏的记者。就像反腐是为了建设更好的中国一样，揭露我们社会中的黑暗是为了让我们的社会更好。那些认为揭露我们社会中的黑暗就是摸黑中国的人，事实上是不爱国的，或者是不理解真正的爱国主义的，因为他们只会让中国停滞不前。
— 来源
同事的一个说法很有趣。他说我们这种底层互害的戾气是有文脉传统的。比如说窦娥被无赖陷害被昏官判死刑，结果窦娥临死许下三桩誓愿：血溅白练，六月飞雪，大旱三年。居然没有一桩是具有逻辑关系的，或者针对自己冤情的制造者的，大旱三年，意味着责任由全天下其它无辜老百姓承担。西游记里凤仙郡一节，郡守夫妻吵架打翻了供品桌，招致玉帝降怒，惩罚也是大旱，要鸡啄完米山蜡烛烧断金锁才给降雨。看下来，玉帝的报复比窦娥还没有逻辑，但特别有权任性特别爽。结果唐僧师徒到凤仙郡时，郡守一家好好的，老百姓十室九空。妈的，讲到这里我已经有点胆战心惊了。在这两个故事里，上位者和下位者的想法出奇一致，都没有就事论事，权责对等，出了事儿受了委屈，都要全天下的人给我付出代价，哪怕大旱三年白骨遍野，也要把心里的气发泄出来。社会，我们所谓公共领域，只是一个许愿撒气的厕所，从来没有获得真正的地位并受到呵护。对于构成社会的其它人，上位者不在乎，失意的下位者们亦是如此。
— 脸叔ak
我是相信，人与人的灵魂是可以共鸣的。当你真的将自己的心注入，从内而外迸发出灵魂的鸣叫时，对面的人是可以通过你的表达(文字，神态，肢体)感受到它的。引伸的一个问题就是：当我用同样的情绪状态，使用文字与ai对话时，它能get 那声灵魂的鸣叫吗？
— 来源
凯特的新片，小李子去捧场了。有时候看到他俩一起出现，就会明白我们年轻时的朋友，在我们生命中存在的意义。因为只有在年轻时相遇的人，才会记得你青春的样子，她在他心中永远是22，他在她心中永远是23。
— 来源
在沃尔玛也有一名同事对我提出忠告：虽然你有很多事情要学，但不要“懂太多”也很重要，永远别让管理阶层了解你到底多有能力，因为“他们越认为你做得到，就会越利用和剥削你”。这些对我提出忠告的前辈们并不是懒散，她们只是很清楚，英雄式的表现几乎不会有任何回报。诀窍在于如何好好分配你的精力，以便还能剩下一些给明天用。
— 来源
员工队伍就好比是厕纸，大多数情况下只有20%的面积是真正起作用的。剩下的80%虽然擦不到屎，但你如果把这多余80%的员工裁掉，那你一定会弄得满手都是。
— 来源
文章 闺女又梦游了！ | 老张博客 碎碎念 | 生活与人生 - 太隐 做了一个只有中国人才能玩的游戏 - V2EX 看见“自然”的眼睛 底层挣扎：穷忙一族的生存之战 ｜芭芭拉·艾伦瑞克 Claude Prompt：细节 帝国的脓疮、崇祯打虎的困局 Casio’s first smart ring has innovative features like a stopwatch and flashing alarm - The Verge Bluetooth 1.0 to 6.0 explained: How do Bluetooth versions differ? TrendX.wiki - Discover Today’s Trending Topics 🤖 [分享贴] 我如何使用 ChatGPT-o1 和 Cursor 在一天全程完成 [ TrendX.wiki ] 的搭建，几乎 0 手写 - V2EX 图片 煎蛋网 No.5797202 煎蛋网 No.5796453 播客 Vol.257 美国的反智传统：为什么越来越多人不相信科学？ - 文化有限 | 小宇宙 （全剧透）罗生黄沙 | 《黑神话：悟空》全剧情解读 | 机核 GCORES 视频 如何快速理解这个世界？15分钟极简人类史！_哔哩哔哩_bilibili 北京师范大学 陈志新 社会科学概论 第02讲 如何读书 读书 温暖和百感交集的旅程 电影 赌博默示录 日记 11月18日 这两天开始，左胳膊稍微有点疼，感觉最近也没干啥剧烈运动啊，真奇怪。 11月19日 下午去图书馆借书，回来的时候还是坐公交，坐车的时候大概下午5点多，赶上了下班高峰期，人很多，我站了十多分钟才做到了最后一排。走了差不多一般的路程，车上的人慢慢的变少了，我看到中后的位置有一个空位，靠窗的位置空着，旁边的位置有个中年男人。我想坐到前面，那个地方有灯，光线很好，我想看看刚刚借的书。我走过去，让那位中年男人让位置，他说你不是快要下车了吗，不情不愿的，不想让位置，我说要么你进去，要么就让我进去，他还是有点慢吞吞的，我又想说点啥，他让开了位置，我就进去坐下了。 坐安稳后，我一边看书，脑子里突然闪出一个构思。故事是这样的，主角最近心情复杂，这天去图书馆借书，坐公交回家，这个中年男人不讲理不让座，主角先讲道理，讲不通道理以后就破口大骂中年男人，中年男人打了主角一拳，主角挨打后又给还了一拳，结果直接把中年男人打昏了，有人报警，有人拍视频，后来警察介入，2个人都被带走了，有人拍摄了全过程，上传到视频平台，结果这段视频火了。其他人都以为这个主角是个普通人的时候，在一个办公室中，某个人也看到了这个视频，发现正是自己找了很多年的人，于是真正的故事开始了。 弟弟向我推荐了空气炸锅，他用着感觉很不错，家里面已经有了，想把原本买给家里的空气炸锅邮寄给我，我让他试试该地址，结果发现改不了，听说需要100多，是他对象买的。我就推辞暂时用不到，需要用的时候我会自己买，感谢推荐。她还是个学生，哪里有那么多闲钱买东西呢，还是不能要的，但还是感谢她的一番心意。有机会我也应该给她买点小礼物。 11月20日 假如我是一个作家，我创作了非常多的角色，这些角色中有一个角色是为我自己量身打造的，我会让读者去猜，猜对有丰厚的奖励。 11月21日 凌晨3点，楼道里的说话声把我吵醒。我目前住的公租房，这地方隔音特别差，我旁边的房子里住了一堆情侣，养了一条小狗，每天晚上10点左右，他们俩会有说有笑的爬楼梯，然后开门，关门，我非常羡慕他们，如果我有一个这样的红颜知己，夫复何求。今天凌晨不知什么原因，2人吵架了，我被吵的睡不着，就劝说他们别吵。我虽然单身27年，但还是能感觉到这种情况，迟早也会发生在我的身上，希望我和她可以更好的解决这种事情吧。 11月22日 我们这些最底层的人，如果丧失表达能力的话，那将非常悲哀。我们的声音可能非常小，但如果我们拥有这种表达能力，但意外来临的时候，我们可以记录这一些，叙述这一切。这应该成为一种基本素质，想吃饭睡觉一样自然。 11月24日 最近都在B站看北京师范大学陈志新的课程，也看到了他推荐的书单，我觉得很不错，以后可以按照他的书单阅读，希望阅读完以后，自己可以知道怎么如何选择阅读，真正的走向阅读之路。 </p>
  </div>
  <footer class="entry-footer"><span title='2024-11-24 19:06:29 +0800 CST'>November 24, 2024</span>&nbsp;·&nbsp;<span>1 min</span>&nbsp;·&nbsp;<span>IT小章</span></footer>
  <a class="entry-link" aria-label="post link to 阅读与感悟周记37" href="http://localhost:1313/weekly-reflections-reading-insights-37/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">ACE安全中心 | 游戏安全组件运行时发生异常 | (1-0-0)
    </h2>
  </header>
  <div class="entry-content">
    <p> 原文地址：https://itxiaozhang.com/ace-security-center-game-component-runtime-error-1-0-0/
如果您需要远程电脑维修或者编程开发，请加我微信咨询。
问题描述 启动腾讯代理的游戏时，部分用户可能遇到“ACE安全中心”报错，错误代码为“1-0-0”。主要表现为：
错误提示：弹窗显示“ACE安全中心”错误，游戏无法启动。 影响范围：包括《无畏契约》等多款腾讯游戏。 解决步骤 打开服务管理器
按Windows键，搜索“服务”，打开“服务”应用。 找到ACE服务
在服务列表中找到与“ACE”相关的服务，双击打开属性窗口。 修改启动类型
将“启动类型”设为“自动”，点击“应用”保存。 启动或重启服务
若服务未启动，点击“启动”；已启动则选择“重新启动”。 重试游戏
完成设置后，重启游戏检查问题是否解决。 错误原因分析 “1-0-0”错误通常因ACE服务未正常启动导致，可能原因包括：
服务被禁用：用户手动关闭或禁用服务。 安全软件干扰：部分安全软件阻止ACE服务运行。 系统配置问题：环境变量异常或任务计划冲突。 视频版本 哔哩哔哩 YouTube </p>
  </div>
  <footer class="entry-footer"><span title='2024-11-24 12:13:35 +0800 CST'>November 24, 2024</span>&nbsp;·&nbsp;<span>1 min</span>&nbsp;·&nbsp;<span>IT小章</span></footer>
  <a class="entry-link" aria-label="post link to ACE安全中心 | 游戏安全组件运行时发生异常 | (1-0-0)" href="http://localhost:1313/ace-security-center-game-component-runtime-error-1-0-0/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Windows 10 局域网共享文件夹设置教程
    </h2>
  </header>
  <div class="entry-content">
    <p> 原文地址：https://itxiaozhang.com/windows-10-local-network-shared-folder-setup-guide/
如果您需要远程电脑维修或者编程开发，请加我微信咨询。
准备工作 确保所有计算机都连接至同一个局域网。可以通过检查每台电脑的IP地址来确认这一点：
使用快捷键 Win &#43; R 调出运行对话框。 输入 cmd 回车，打开命令提示符。 输入 ipconfig 回车，查看每台电脑的IPv4地址，确保它们属于同一个子网。 启用网络发现和文件共享 在每台电脑上执行以下步骤：
使用快捷键 Win &#43; I 打开设置。 选择“网络和Internet” &gt; “状态” &gt; “网络和共享中心”。 在左侧菜单中点击“更改高级共享设置”。 选择当前使用的网络配置文件（如家庭或工作），勾选“启用网络发现”和“启用文件和打印机共享”。 点击“保存更改”。 设置共享文件夹 选择共享文件夹：
在任一电脑上，选择一个合适的文件夹作为共享文件夹。建议新建文件夹，以避免影响系统盘的数据。例如，在D盘新建一个名为“共享文件夹”的文件夹。 设置共享权限：
右键点击选择的文件夹，选择“属性”。 转到“共享”选项卡，点击“高级共享”。 勾选“共享此文件夹”，点击“权限”。 在权限设置中，选择“Everyone”，赋予“读取”或“读取/写入”权限，根据实际需求调整。 点击“应用” &gt; “确定”完成设置。 访问共享文件夹：
在其他电脑上，使用快捷键 Win &#43; R 调出运行对话框。 输入 \\&lt;共享电脑的IP地址&gt;\共享文件夹名称，例如 \\192.168.1.109\共享文件夹。 第一次访问可能需要输入共享文件夹所在电脑的用户名和密码。 报错案例：账户已锁定 如果在设置共享文件夹过程中遇到账户已锁定的问题，可以通过以下步骤解决：
使用管理员账户登录：
确保你使用的是具有管理员权限的账户登录。 打开计算机管理：
按 Win &#43; X 键，选择“计算机管理”。 找到被锁定的账户：
在左侧导航栏中，依次展开“本地用户和组” &gt; “用户”。 找到被锁定的账户，右键点击该账户，选择“属性”。 取消账户锁定：
在“常规”选项卡下，取消选中“账户已禁用”复选框。 在“账户已锁定”部分，取消选中“账户已锁定”复选框。 点击“应用” &gt; “确定”保存设置。 </p>
  </div>
  <footer class="entry-footer"><span title='2024-11-21 13:38:48 +0800 CST'>November 21, 2024</span>&nbsp;·&nbsp;<span>1 min</span>&nbsp;·&nbsp;<span>IT小章</span></footer>
  <a class="entry-link" aria-label="post link to Windows 10 局域网共享文件夹设置教程" href="http://localhost:1313/windows-10-local-network-shared-folder-setup-guide/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">GitHub Actions 自动部署：配置 GitHub 仓库指定文件夹到 Ubuntu 服务器
    </h2>
  </header>
  <div class="entry-content">
    <p> 原文地址：https://itxiaozhang.com//github-actions-sync-specific-folder-to-ubuntu-server-complete-guide/
如果您需要远程电脑维修或者编程开发，请加我微信咨询。
需求分析 源：
GitHub 仓库：[你的仓库地址] 需同步的文件夹：public/ 目标：
服务器：[服务器IP] SSH 端口：[SSH端口] 用户：[用户名] 目标路径：/var/www/html/[你的域名] 功能要求：
当 push 到 main 分支时自动部署 只同步 public 文件夹内容 部署前清空目标目录 无需构建步骤 无需部署后操作 工作流程 生成新的 public 文件夹中的文件 提交并推送到 GitHub GitHub Actions 自动触发 Actions 通过 SSH 连接服务器 清空目标目录 同步新文件到服务器 详细教程 1. 服务器配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 1.1 SSH 登录服务器 ssh [用户名]@[服务器IP] -p [SSH端口] # 1.2 生成 SSH 密钥对 ssh-keygen -t ed25519 -C &#34;github-actions-deploy&#34; # 按三次回车，使用默认设置 # 1.3 设置 SSH 目录权限 mkdir -p ~/.ssh chmod 700 ~/.ssh # 1.4 配置授权 cat ~/.ssh/id_ed25519.pub &gt;&gt; ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys # 1.5 查看私钥（需要复制这个内容） vim ~/.ssh/id_ed25519 # 使用 y 复制内容 # 使用 :q 退出 # 1.6 创建并设置部署目录 mkdir -p /var/www/html/[你的域名] chmod 755 /var/www/html/[你的域名] 2. GitHub 配置 添加 Repository Secrets
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-11-19 23:10:18 +0800 CST'>November 19, 2024</span>&nbsp;·&nbsp;<span>2 min</span>&nbsp;·&nbsp;<span>IT小章</span></footer>
  <a class="entry-link" aria-label="post link to GitHub Actions 自动部署：配置 GitHub 仓库指定文件夹到 Ubuntu 服务器" href="http://localhost:1313/github-actions-sync-specific-folder-to-ubuntu-server-complete-guide/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">阅读与感悟周记36
    </h2>
  </header>
  <div class="entry-content">
    <p> 原文地址：https://itxiaozhang.com/weekly-reflections-reading-insights-36/
如果您需要远程电脑维修或者编程开发，请加我微信咨询。
文章 秉承Build in Public，分享下使用Cursor实 - 即刻App 最近悟到了一个顶级心态，如何拿回自己的主体性。 - 即刻App 开源精神都贯彻落实不到位的国人，就不要想着网上那些可以带你赚钱的人了 - V2EX 挑战:第一次出国挑战，明年 4 月 想去日本，我需要做什么准备 - V2EX 用类比的方式点评下神级 Prompt，以及它和 o1 推理模型的差距 | 宝玉的分享 能够留守一扇窗口才是幸运 年轻人要存钱，尽力多存，有多少存多少 - 即刻App 用 AI 做了一个传统企业网站 - V2EX 提示词是越详尽越好吗，还是更加富有创造性？ | 宝玉的分享 只是上学，并不能帮你弄懂世界的运行规则 - 即刻App 图片 剑桥Girton/吉尔顿村庄秋天风景照 西柚色的天空 好柿连连 播客 Ep 50. 独立开发，做 App 还是做 SaaS？【开发篇】 - 捕蛇者说 | 小宇宙 E21. 比特币$7.5w再次创新高, 普通人可用的3种BTC投资方法 w/ 倪森 Phyrex - Day1Global生而全球 | 小宇宙 以史明今：从科技革命的规律看 AI 时代的机会｜S8E24 - What’s Next｜科技早知道 | 小宇宙 E23. 如何帮富豪们布局加密货币，特朗普当选的影响，未来什么值得买 w/ EO Hao - Day1Global生而全球 | 小宇宙 视频 满大街跑的集装箱你了解有多少？_哔哩哔哩 被所有人小瞧的铁皮盒子究竟能有多厉害？【有机社会】_哔哩哔哩 【隐居之地】三万六千人民币的房子可以住吗？_哔哩哔哩 看书 梅雨 电影 幸福的国家 灰猎犬号 全程紧张刺激，代入感很强。拍摄很写实，演技扎实。做一个出色的船长真不容易，不眠不休，反应迅速，随时要下达命令，随时做出重大决定。 我有个疑问，这么大的运输船队，装的物资非常多，运输船队的武器也很厉害，为什么没有配备更多的空中支援呢？ 人类互相侵略是否写在基因中的呢，这种邪恶难道真的是无法祛除的吗？ 剧集 黑镜 第二季 言论 “我觉得疫情是一个转折点，现在世界正在退缩，仿佛被拖回到过去。我甚至可以说，世界变得越来越中世纪化了。全球化正在大规模退缩，而曾经充满希望的社交媒体也似乎走到了尽头。一座被高墙围绕的城市的景象可能反映了这种被阻碍和封闭的状况。也许在我们生活的这个时代，古老的故事可能会产生一种意想不到的共鸣。我对这种可能性充满希望。”
— 村上春树，《纽约客》采访
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-11-17 18:31:12 +0800 CST'>November 17, 2024</span>&nbsp;·&nbsp;<span>1 min</span>&nbsp;·&nbsp;<span>IT小章</span></footer>
  <a class="entry-link" aria-label="post link to 阅读与感悟周记36" href="http://localhost:1313/weekly-reflections-reading-insights-36/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">常见电脑网络问题解决思路
    </h2>
  </header>
  <div class="entry-content">
    <p> 原文地址：https://itxiaozhang.com/common-computer-network-problems-solutions/
如果您需要远程电脑维修或者编程开发，请加我微信咨询。
前言 本指南适用于Windows 7、Windows 10 和 Windows 11 系统的电脑，可以排查和解决常见的网络问题。
使用说明 建议按照顺序依次排查。 建议按照以下方法逐一测试。 尝试一种方式后，先测试网络是否恢复，问题解决后可以忽略其他步骤。 基础操作中的5种方法，可以应对常见网络问题，如果你想快速解决问题，而不想纠结原因，可以直接开始基础操作。 基础操作 1. 重启电脑 重启是解决网络问题的简单方法，很多小故障在重启后即可恢复。
2. 重新拔插网线 / 重新连接Wi-Fi 简单的网线重新连接或Wi-Fi重新连接有时也能恢复网络连接。
3. 禁用或启用网卡 按 Win &#43; R 打开运行窗口。 输入 ncpa.cpl 并点击确定。 在网络连接列表中找到你的网卡（通常是“以太网”或“WiFi”）。 右键选择“禁用”，然后再右键选择“启用”。 4. 重置网络 按 Win &#43; X 键，选择“命令提示符（管理员）”或“Windows PowerShell（管理员）”。 在命令提示符中输入 netsh winsock reset 并按 Enter。 系统提示重启电脑以完成重置，请保存工作后重启。 5. 卸载并重装网卡驱动 右键点击 Windows 图标，选择“设备管理器”。 展开“网络适配器”类别，找到你的网卡。 右键点击网卡，选择“卸载设备”。 重启电脑，系统会自动重新安装网卡驱动。 进阶操作 大部分网络问题通过基础操作可以解决，但当基础操作无效时，可以尝试以下进阶方法，请对号入座。
1. 软件可以联网，但浏览器无法访问网页，或者无法访问单个网页 分析：此情况多半是DNS问题。 解决：更改DNS服务器地址，例如8.8.8.8（谷歌DNS），具体操作可在网络连接的属性设置中进行。 2. Wi-Fi连接正常但无法使用网络 分析：可能是路由器或光猫的问题。 解决方案： 断电重启光猫或路由器：直接断电重启设备，如无效可尝试复位，复位前确保已备份重要配置。 使用PE系统测试：使用带网络的PE系统检查是否为系统问题，必要时考虑系统还原或重装。 3. 蓝牙 / WiFi 图标消失 分析：有可能是静电引起。 解决：关机后按住电源键30秒释放静电，再开机测试。 4. 无法联网且USB网络共享也无法使用 分析：可能是注册表或系统设置问题。 解决：可使用带网络功能的PE系统排查，若硬件正常，尝试系统还原或重新安装系统。 5. 硬件问题 如果以上方法均无效，有可能是网卡硬件故障：
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-11-16 13:29:24 +0800 CST'>November 16, 2024</span>&nbsp;·&nbsp;<span>1 min</span>&nbsp;·&nbsp;<span>IT小章</span></footer>
  <a class="entry-link" aria-label="post link to 常见电脑网络问题解决思路" href="http://localhost:1313/common-computer-network-problems-solutions/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">安卓手机USB网络共享给Windows
    </h2>
  </header>
  <div class="entry-content">
    <p> 原文地址：https://itxiaozhang.com/how-to-share-android-phone-internet-with-windows-via-usb/
如果您需要远程电脑维修或者编程开发，请加我微信咨询。
准备工作 一台安卓手机。 一台Windows电脑。 安卓手机数据线，最好是原装数据线。 步骤1：连接手机与电脑 使用USB数据线将安卓手机和Windows电脑连起来。 步骤2：启用USB网络共享 打开手机“设置”。 搜索“网络共享”。 找到“USB网络共享”。 开启“USB网络共享”开关。 如果搜不到“USB网络共享”，请看步骤3。 步骤3：启用USB调试（备用） 打开手机“设置”。 向下滚动到底部，选择“关于手机”。 找到“软件信息”，然后点击“版本号”七次，以激活开发者选项。 继续步骤2。 步骤4：在Windows上设置网络 默认会直接自动连接，如果已经连接，就忽略以下内容。 如果没有连接，在网络适配器页面找到名为“USB Ethernet/RNDIS”或类似的网络连接，右键点击它，然后选择“启用”。 步骤5：测试网络连接 打开浏览器，尝试访问网页，看看是否可以正常上网。 如果不能上网，检查手机上的USB网络共享是否已开启，或者更换数据线。 尝试重新启动手机和电脑，有时候这能解决连接问题。 视频版本 哔哩哔哩 YouTube </p>
  </div>
  <footer class="entry-footer"><span title='2024-11-13 13:07:27 +0800 CST'>November 13, 2024</span>&nbsp;·&nbsp;<span>1 min</span>&nbsp;·&nbsp;<span>IT小章</span></footer>
  <a class="entry-link" aria-label="post link to 安卓手机USB网络共享给Windows" href="http://localhost:1313/how-to-share-android-phone-internet-with-windows-via-usb/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="prev" href="http://localhost:1313/page/13/">
      «&nbsp;Prev&nbsp;
    </a>
    <a class="next" href="http://localhost:1313/page/15/">Next&nbsp;&nbsp;»
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">IT小章</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
