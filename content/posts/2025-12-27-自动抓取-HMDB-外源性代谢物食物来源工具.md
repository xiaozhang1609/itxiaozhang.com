---
title: '自动抓取 HMDB 外源性代谢物食物来源工具'
url: auto-hmdb-exogenous-metabolite-food-source
date: 2025-12-27T20:53:32+08:00
description: 自动化抓取 HMDB 数据库中代谢物外源性来源，生成 CSV，便于科研与食物-代谢物关联分析。
categories:
  - 编程开发
tags:
  - HMDB
  - hmdb数据库
  - python
  - 代谢物
author: IT小章
---

> 原文地址：<https://itxiaozhang.com/auto-hmdb-exogenous-metabolite-food-source>  
> 如果您需要远程电脑维修或者编程开发，请[加我微信](https://zhang9.cn)咨询。 


## 1. 需求分析

代谢物的来源信息对于科研和营养分析非常重要。现有数据库（HMDB）中，外源性代谢物来源信息分散在网页上，手动整理效率低、容易出错。因此，需要一个工具能批量抓取代谢物的外源性来源信息，生成结构化、可分析的 CSV 数据。

## 2. 工具功能概述

* **抓取范围**：只关注 HMDB 中 Disposition → Exogenous，即代谢物外源性来源。
* **输出数据**：CSV 文件，每条记录包含 HMDB ID、食物名称、FooDB 编号；找不到或出错标 `None`。
* **自动化与鲁棒性**：

  * 多线程并发抓取，提高效率
  * 随机 User-Agent、防封锁
  * 自动重试和错误处理
  * 每处理若干条自动保存，保证数据安全
* **用途**：支持食物-代谢物关联分析、营养研究及数据库构建。

## 3. 程序结构

核心程序通过 Python 实现，主要功能包括：

* 加载 HMDB ID 列表
* 请求对应网页
* 提取代谢物外源性食物来源信息
* 生成 CSV 文件，每条记录包括 HMDB ID、食物名称及对应编号
* 错误处理和日志记录，确保每个 ID 都有处理结果

示意代码如下：

```python
import csv
import time
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from lxml import html
import re
import html as html_module
import os
from collections import Counter
import sys
import random

# 作者：IT小章
# 时间：2025年12月27日
# 网站：itxiaozhang.com

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Safari/537.36'
]

def get_random_headers():
    return {
        'User-Agent': random.choice(USER_AGENTS)
    }

def check_ids(ids):
    invalid_ids = []
    id_counts = Counter(ids)
    duplicates = {id: count for id, count in id_counts.items() if count > 1}
    
    for index, id in enumerate(ids, 1):
        if not id.startswith("HMDB"):
            invalid_ids.append((index, id))
    
    return invalid_ids, duplicates

def clean_text(text):
    # 移除不可见字符和特殊字符
    cleaned = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F\u200B-\u200D\uFEFF]', '', text)
    # 将多个空格替换为单个空格
    cleaned = re.sub(r'\s+', ' ', cleaned)
    return cleaned.strip()

def get_metabolite_data(hmdb_id):
    url = f"https://hmdb.ca/metabolites/{hmdb_id}"
    
    # 增加随机延迟，避免请求过快
    time.sleep(random.uniform(1, 3))
    
    response = requests.get(url, headers=get_random_headers(), timeout=30)
    response.raise_for_status()
    
    tree = html.fromstring(response.content)

    data = {'HMDB ID': hmdb_id}

    # 获取 Disposition
    disposition_nodes = tree.xpath("/html/body/main/table/tbody[1]/tr[35]/td")
    
    exogenous_info = []
    if disposition_nodes:
        # 严格定位 Source -> Exogenous 路径
        # 1. 找到 "Source" 节点
        # 2. 在其子节点中查找 "Exogenous"
        # 3. 在 Exogenous 下查找所有包含 FooDB 链接的 li 节点
        
        # 使用 xpath 查找所有位于 "Exogenous" 类别下的 li 节点
        # 这里的路径逻辑：
        # //a[text()='Exogenous']/following-sibling::ul//li//a[contains(@href, 'foodb.ca/foods/')]
        # 解释：找到文本为 Exogenous 的链接 -> 找它后面的 ul 列表 -> 找里面的 li -> 找指向 foodb 的链接
        
        # 注意：源码中 Exogenous 出现了两次，一次是作为父级分类，一次是具体的 Exogenous 描述
        # 我们需要确保是在 Source 下的 Exogenous
        
        targets = disposition_nodes[0].xpath(".//a[text()='Source']/following-sibling::ul//a[text()='Exogenous']/following-sibling::ul//a[contains(@href, 'foodb.ca/foods/')]")
        
        for food_node in targets:
             # 获取 FooDB ID
            foodb_id = food_node.text.strip() if food_node.text else ""
            
            # 获取食物名称
            food_name_node = food_node.xpath("./preceding::a[1]")
            if food_name_node:
                food_name = food_name_node[0].text.strip()
                if food_name and foodb_id:
                     exogenous_info.append(f"{food_name} ({foodb_id})")

    # 如果没有找到特定格式的数据，填写 "None"
    data['Source(Exogenous)'] = '\n'.join(exogenous_info) if exogenous_info else 'None'

    return data

def get_metabolite_data_with_retry(hmdb_id, max_retries=3):
    for attempt in range(max_retries):
        try:
            return get_metabolite_data(hmdb_id)
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 404:
                return {'HMDB ID': hmdb_id, 'Source(Exogenous)': 'ID Not Found (404)'}
            if attempt == max_retries - 1:
                return {'HMDB ID': hmdb_id, 'Source(Exogenous)': f'HTTP Error: {e}'}
            time.sleep(2 ** attempt + random.uniform(0, 1))
        except Exception as e:
            if attempt == max_retries - 1:
                print(f"[{hmdb_id}] 所有重试尝试均失败。错误: {e}")
                return {'HMDB ID': hmdb_id, 'Source(Exogenous)': f'Error: {e}'}
            else:
                print(f"[{hmdb_id}] 尝试 {attempt + 1} 失败，准备重试... 错误: {e}")
            time.sleep(2 ** attempt + random.uniform(0, 1))
    return {'HMDB ID': hmdb_id, 'Source(Exogenous)': 'Failed after retries'}

def write_to_csv(results, filename, mode='a'):
    fieldnames = [
        'HMDB ID',
        'Source(Exogenous)'
    ]
    
    file_exists = os.path.isfile(filename)
    
    with open(filename, mode, newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        if not file_exists or mode == 'w':
            writer.writeheader()
        writer.writerows(results)

def process_ids(hmdb_ids, max_workers=5):
    results = {}
    total_ids = len(hmdb_ids)
    processed_count = 0
    success_count = 0
    failure_count = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_id = {executor.submit(get_metabolite_data_with_retry, hmdb_id): hmdb_id for hmdb_id in hmdb_ids}
        
        for future in as_completed(future_to_id):
            hmdb_id = future_to_id[future]
            processed_count += 1
            try:
                data = future.result()
                # 只要返回数据（包括错误信息），都视为处理完成并保存
                if data:
                    results[hmdb_id] = data
                    # 如果数据中包含错误信息，不算作"成功"抓取，但仍然保存
                    if 'Error' in data.get('Source(Exogenous)', '') or 'Not Found' in data.get('Source(Exogenous)', ''):
                        status = f"失败 ({data['Source(Exogenous)']})"
                        failure_count += 1
                    else:
                        status = "成功"
                        success_count += 1
                else:
                    # 理论上不应到达这里，因为 get_metabolite_data_with_retry 现在总是返回字典
                    results[hmdb_id] = {'HMDB ID': hmdb_id, 'Source(Exogenous)': 'Unknown Error'}
                    status = "失败 (Unknown Error)"
                    failure_count += 1
            except Exception as e:
                results[hmdb_id] = {'HMDB ID': hmdb_id, 'Source(Exogenous)': f'Process Error: {e}'}
                status = f"失败 (Process Error: {e})"
                failure_count += 1
            
            print(f"正在处理第 {processed_count}/{total_ids} 个: {hmdb_id} - {status}")
            
            if processed_count % 10 == 0:
                write_to_csv(list(results.values()), '代谢物数据_最终.csv', mode='a')
                print(f"已保存到最终文件。已处理: {processed_count}, 成功: {success_count}, 失败: {failure_count}")
                results.clear()
    
    if results:
        write_to_csv(list(results.values()), '代谢物数据_最终.csv', mode='a')
    
    return success_count, failure_count

def main():
    try:
        if not os.path.exists('hmid.txt'):
            print("错误：找不到 hmid.txt 文件。请确保该文件与程序在同一目录下。")
            input("按回车键退出...")
            sys.exit(1)

        with open('hmid.txt', 'r') as f:
            hmdb_ids = f.read().splitlines()
        print(f"从hmid.txt加载了 {len(hmdb_ids)} 个ID")
        
        # invalid_ids, duplicates = check_ids(hmdb_ids)
        
        # if invalid_ids:
        #     print("发现以下异常ID:")
        #     for index, id in invalid_ids:
        #         print(f"  第{index}行: {id}")
        
        # if duplicates:
        #     print("发现以重复ID:")
        #     for id, count in duplicates.items():
        #         print(f"  {id}: 重复{count}次")
        
        # 即使ID无效或重复，也应处理以保证输出数量一致
        # 但为了避免完全错误的请求，我们还是过滤一下非HMDB开头的，或者对其标记错误
        
        all_ids = hmdb_ids
        
        print(f"将处理 {len(all_ids)} 个ID")
    except Exception as e:
        print(f"发生错误: {e}")
    finally:
        write_to_csv([], '代谢物数据_最终.csv', mode='w')

        success_count, failure_count = process_ids(all_ids)

        print(f"数据提取完成。总计: {len(all_ids)}, 成功: {success_count}, 失败: {failure_count}")
        print(f"最终结果已保存在代谢物数据_最终.csv中")

        # print("按回车键退出程序...")
        # input()

if __name__ == "__main__":
    main()

```


## 视频版本

* [哔哩哔哩](https://space.bilibili.com/3546607630944387)
* [YouTube](https://www.youtube.com/@itxiaozhang)


